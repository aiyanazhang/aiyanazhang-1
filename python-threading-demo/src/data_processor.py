"""
æ•°æ®å¤„ç†å™¨å®é™…åº”ç”¨åœºæ™¯æ¼”ç¤º
å¤§æ•°æ®é›†å¹¶è¡Œå¤„ç†ï¼Œå±•ç¤ºå¤šçº¿ç¨‹åœ¨æ•°æ®å¤„ç†ä¸­çš„åº”ç”¨
"""

import threading
import time
import random
import csv
import json
import os
from datetime import datetime, timedelta
from typing import List, Dict, Any, Callable, Optional, Tuple
import concurrent.futures
import queue
from collections import defaultdict
import math


class DataChunk:
    """æ•°æ®å—ç±»"""
    
    def __init__(self, chunk_id: str, data: List[Dict[str, Any]], chunk_size: int):
        self.chunk_id = chunk_id
        self.data = data
        self.chunk_size = chunk_size
        self.processed = False
        self.result: Optional[Dict[str, Any]] = None
        self.processing_time: float = 0.0
        self.error: Optional[str] = None
        
    def __len__(self):
        return len(self.data)


class DataGenerator:
    """æµ‹è¯•æ•°æ®ç”Ÿæˆå™¨"""
    
    @staticmethod
    def generate_sales_data(count: int) -> List[Dict[str, Any]]:
        """ç”Ÿæˆé”€å”®æ•°æ®"""
        products = ['ç¬”è®°æœ¬ç”µè„‘', 'æ‰‹æœº', 'å¹³æ¿ç”µè„‘', 'è€³æœº', 'é”®ç›˜', 'é¼ æ ‡', 'æ˜¾ç¤ºå™¨', 'éŸ³å“']
        regions = ['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·', 'æ·±åœ³', 'æ­å·', 'æˆéƒ½', 'æ­¦æ±‰', 'è¥¿å®‰']
        
        data = []
        base_date = datetime(2023, 1, 1)
        
        for i in range(count):
            record = {
                'id': f"ORDER_{i+1:06d}",
                'product': random.choice(products),
                'region': random.choice(regions),
                'quantity': random.randint(1, 100),
                'unit_price': round(random.uniform(100, 5000), 2),
                'date': (base_date + timedelta(days=random.randint(0, 365))).isoformat(),
                'customer_id': f"CUST_{random.randint(1, 10000):05d}",
                'sales_person': f"SALES_{random.randint(1, 50):03d}"
            }
            record['total_amount'] = record['quantity'] * record['unit_price']
            data.append(record)
        
        return data
    
    @staticmethod
    def generate_log_data(count: int) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ—¥å¿—æ•°æ®"""
        log_levels = ['INFO', 'WARNING', 'ERROR', 'DEBUG']
        services = ['user-service', 'order-service', 'payment-service', 'inventory-service']
        
        data = []
        base_time = datetime.now() - timedelta(days=7)
        
        for i in range(count):
            record = {
                'timestamp': (base_time + timedelta(
                    seconds=random.randint(0, 7*24*3600)
                )).isoformat(),
                'level': random.choice(log_levels),
                'service': random.choice(services),
                'message': f"Operation {i+1} executed",
                'user_id': random.randint(1, 1000),
                'session_id': f"SESSION_{random.randint(1, 5000):05d}",
                'response_time': random.randint(10, 2000),  # ms
                'status_code': random.choice([200, 201, 400, 404, 500])
            }
            data.append(record)
        
        return data


class DataProcessor:
    """æ•°æ®å¤„ç†å™¨ç±»"""
    
    def __init__(self, max_workers: int = 4, chunk_size: int = 1000):
        self.max_workers = max_workers
        self.chunk_size = chunk_size
        self.processing_stats = {
            'total_records': 0,
            'processed_records': 0,
            'chunks_processed': 0,
            'start_time': None,
            'end_time': None,
            'errors': []
        }
        self.stats_lock = threading.Lock()
        
        print(f"ğŸ”§ æ•°æ®å¤„ç†å™¨é…ç½®:")
        print(f"   æœ€å¤§å·¥ä½œçº¿ç¨‹: {max_workers}")
        print(f"   æ•°æ®å—å¤§å°: {chunk_size}")
    
    def split_data(self, data: List[Dict[str, Any]]) -> List[DataChunk]:
        """å°†æ•°æ®åˆ†å‰²æˆå—"""
        chunks = []
        total_records = len(data)
        
        for i in range(0, total_records, self.chunk_size):
            chunk_data = data[i:i + self.chunk_size]
            chunk_id = f"CHUNK_{len(chunks)+1:03d}"
            chunk = DataChunk(chunk_id, chunk_data, len(chunk_data))
            chunks.append(chunk)
        
        print(f"ğŸ“Š æ•°æ®åˆ†å‰²å®Œæˆ: {total_records:,} æ¡è®°å½• â†’ {len(chunks)} ä¸ªæ•°æ®å—")
        return chunks
    
    def process_sales_analytics(self, chunk: DataChunk) -> Dict[str, Any]:
        """å¤„ç†é”€å”®æ•°æ®åˆ†æ"""
        start_time = time.time()
        
        try:
            print(f"[{threading.current_thread().name}] å¼€å§‹å¤„ç† {chunk.chunk_id} ({len(chunk)} æ¡è®°å½•)")
            
            # æ¨¡æ‹Ÿå¤æ‚çš„æ•°æ®å¤„ç†
            time.sleep(random.uniform(0.5, 1.5))
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            total_amount = sum(record['total_amount'] for record in chunk.data)
            total_quantity = sum(record['quantity'] for record in chunk.data)
            
            # æŒ‰äº§å“åˆ†ç»„ç»Ÿè®¡
            product_stats = defaultdict(lambda: {'quantity': 0, 'amount': 0, 'orders': 0})
            for record in chunk.data:
                product = record['product']
                product_stats[product]['quantity'] += record['quantity']
                product_stats[product]['amount'] += record['total_amount']
                product_stats[product]['orders'] += 1
            
            # æŒ‰åœ°åŒºåˆ†ç»„ç»Ÿè®¡
            region_stats = defaultdict(lambda: {'quantity': 0, 'amount': 0, 'orders': 0})
            for record in chunk.data:
                region = record['region']
                region_stats[region]['quantity'] += record['quantity']
                region_stats[region]['amount'] += record['total_amount']
                region_stats[region]['orders'] += 1
            
            # è®¡ç®—å¹³å‡å€¼
            avg_order_value = total_amount / len(chunk.data) if chunk.data else 0
            avg_quantity = total_quantity / len(chunk.data) if chunk.data else 0
            
            result = {
                'chunk_id': chunk.chunk_id,
                'record_count': len(chunk.data),
                'total_amount': total_amount,
                'total_quantity': total_quantity,
                'avg_order_value': avg_order_value,
                'avg_quantity': avg_quantity,
                'product_stats': dict(product_stats),
                'region_stats': dict(region_stats),
                'processing_thread': threading.current_thread().name
            }
            
            chunk.result = result
            chunk.processed = True
            chunk.processing_time = time.time() - start_time
            
            print(f"[{threading.current_thread().name}] âœ… {chunk.chunk_id} å¤„ç†å®Œæˆ "
                  f"(è€—æ—¶: {chunk.processing_time:.2f}s)")
            
            return result
            
        except Exception as e:
            chunk.error = str(e)
            chunk.processing_time = time.time() - start_time
            print(f"[{threading.current_thread().name}] âŒ {chunk.chunk_id} å¤„ç†å¤±è´¥: {e}")
            raise
    
    def process_log_analytics(self, chunk: DataChunk) -> Dict[str, Any]:
        """å¤„ç†æ—¥å¿—æ•°æ®åˆ†æ"""
        start_time = time.time()
        
        try:
            print(f"[{threading.current_thread().name}] å¼€å§‹åˆ†æ {chunk.chunk_id} ({len(chunk)} æ¡æ—¥å¿—)")
            
            # æ¨¡æ‹Ÿæ—¥å¿—åˆ†æå¤„ç†
            time.sleep(random.uniform(0.3, 1.0))
            
            # ç»Ÿè®¡å„ç§æŒ‡æ ‡
            level_counts = defaultdict(int)
            service_counts = defaultdict(int)
            status_counts = defaultdict(int)
            response_times = []
            error_logs = []
            
            for record in chunk.data:
                level_counts[record['level']] += 1
                service_counts[record['service']] += 1
                status_counts[record['status_code']] += 1
                response_times.append(record['response_time'])
                
                if record['level'] == 'ERROR':
                    error_logs.append(record)
            
            # è®¡ç®—å“åº”æ—¶é—´ç»Ÿè®¡
            avg_response_time = sum(response_times) / len(response_times) if response_times else 0
            max_response_time = max(response_times) if response_times else 0
            min_response_time = min(response_times) if response_times else 0
            
            result = {
                'chunk_id': chunk.chunk_id,
                'log_count': len(chunk.data),
                'level_distribution': dict(level_counts),
                'service_distribution': dict(service_counts),
                'status_distribution': dict(status_counts),
                'avg_response_time': avg_response_time,
                'max_response_time': max_response_time,
                'min_response_time': min_response_time,
                'error_count': len(error_logs),
                'processing_thread': threading.current_thread().name
            }
            
            chunk.result = result
            chunk.processed = True
            chunk.processing_time = time.time() - start_time
            
            print(f"[{threading.current_thread().name}] âœ… {chunk.chunk_id} åˆ†æå®Œæˆ "
                  f"(è€—æ—¶: {chunk.processing_time:.2f}s)")
            
            return result
            
        except Exception as e:
            chunk.error = str(e)
            chunk.processing_time = time.time() - start_time
            print(f"[{threading.current_thread().name}] âŒ {chunk.chunk_id} åˆ†æå¤±è´¥: {e}")
            raise
    
    def parallel_processing(self, data: List[Dict[str, Any]], 
                          processor_func: Callable[[DataChunk], Dict[str, Any]]) -> Dict[str, Any]:
        """å¹¶è¡Œå¤„ç†æ•°æ®"""
        self.processing_stats['total_records'] = len(data)
        self.processing_stats['start_time'] = time.time()
        
        print(f"\nğŸš€ å¼€å§‹å¹¶è¡Œå¤„ç† {len(data):,} æ¡æ•°æ®")
        print("=" * 60)
        
        # åˆ†å‰²æ•°æ®
        chunks = self.split_data(data)
        
        # å¹¶è¡Œå¤„ç†
        results = []
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰å¤„ç†ä»»åŠ¡
            future_to_chunk = {
                executor.submit(processor_func, chunk): chunk 
                for chunk in chunks
            }
            
            # æ”¶é›†ç»“æœ
            completed_chunks = 0
            total_chunks = len(chunks)
            
            for future in concurrent.futures.as_completed(future_to_chunk):
                chunk = future_to_chunk[future]
                try:
                    result = future.result()
                    results.append(result)
                    completed_chunks += 1
                    
                    with self.stats_lock:
                        self.processing_stats['processed_records'] += len(chunk)
                        self.processing_stats['chunks_processed'] += 1
                    
                    progress = (completed_chunks / total_chunks) * 100
                    print(f"ğŸ“Š å¤„ç†è¿›åº¦: {completed_chunks}/{total_chunks} ({progress:.1f}%)")
                    
                except Exception as e:
                    with self.stats_lock:
                        self.processing_stats['errors'].append({
                            'chunk_id': chunk.chunk_id,
                            'error': str(e),
                            'timestamp': datetime.now().isoformat()
                        })
                    print(f"âŒ å—å¤„ç†å¤±è´¥: {chunk.chunk_id} - {e}")
        
        self.processing_stats['end_time'] = time.time()
        
        # èšåˆç»“æœ
        aggregated_result = self._aggregate_results(results, chunks)
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        self._print_processing_summary(chunks)
        
        return aggregated_result
    
    def _aggregate_results(self, results: List[Dict[str, Any]], 
                          chunks: List[DataChunk]) -> Dict[str, Any]:
        """èšåˆå¤„ç†ç»“æœ"""
        if not results:
            return {}
        
        # æ£€æŸ¥ç»“æœç±»å‹
        if 'total_amount' in results[0]:
            return self._aggregate_sales_results(results)
        elif 'log_count' in results[0]:
            return self._aggregate_log_results(results)
        else:
            return {'message': 'æœªçŸ¥çš„ç»“æœç±»å‹'}
    
    def _aggregate_sales_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """èšåˆé”€å”®æ•°æ®ç»“æœ"""
        total_amount = sum(r['total_amount'] for r in results)
        total_quantity = sum(r['total_quantity'] for r in results)
        total_records = sum(r['record_count'] for r in results)
        
        # èšåˆäº§å“ç»Ÿè®¡
        global_product_stats = defaultdict(lambda: {'quantity': 0, 'amount': 0, 'orders': 0})
        for result in results:
            for product, stats in result['product_stats'].items():
                global_product_stats[product]['quantity'] += stats['quantity']
                global_product_stats[product]['amount'] += stats['amount']
                global_product_stats[product]['orders'] += stats['orders']
        
        # èšåˆåœ°åŒºç»Ÿè®¡
        global_region_stats = defaultdict(lambda: {'quantity': 0, 'amount': 0, 'orders': 0})
        for result in results:
            for region, stats in result['region_stats'].items():
                global_region_stats[region]['quantity'] += stats['quantity']
                global_region_stats[region]['amount'] += stats['amount']
                global_region_stats[region]['orders'] += stats['orders']
        
        # æ‰¾å‡ºé”€é‡æœ€é«˜çš„äº§å“å’Œåœ°åŒº
        top_product = max(global_product_stats.items(), key=lambda x: x[1]['amount'])
        top_region = max(global_region_stats.items(), key=lambda x: x[1]['amount'])
        
        return {
            'summary': {
                'total_records': total_records,
                'total_amount': total_amount,
                'total_quantity': total_quantity,
                'avg_order_value': total_amount / total_records if total_records > 0 else 0,
                'avg_quantity': total_quantity / total_records if total_records > 0 else 0
            },
            'top_performers': {
                'product': {'name': top_product[0], 'stats': top_product[1]},
                'region': {'name': top_region[0], 'stats': top_region[1]}
            },
            'product_analysis': dict(global_product_stats),
            'region_analysis': dict(global_region_stats),
            'chunks_processed': len(results)
        }
    
    def _aggregate_log_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """èšåˆæ—¥å¿—æ•°æ®ç»“æœ"""
        total_logs = sum(r['log_count'] for r in results)
        total_errors = sum(r['error_count'] for r in results)
        
        # èšåˆå„ç§åˆ†å¸ƒ
        global_level_dist = defaultdict(int)
        global_service_dist = defaultdict(int)
        global_status_dist = defaultdict(int)
        
        all_response_times = []
        
        for result in results:
            for level, count in result['level_distribution'].items():
                global_level_dist[level] += count
            
            for service, count in result['service_distribution'].items():
                global_service_dist[service] += count
            
            for status, count in result['status_distribution'].items():
                global_status_dist[status] += count
            
            # æ”¶é›†å“åº”æ—¶é—´ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥ä»åŸå§‹æ•°æ®è®¡ç®—ï¼‰
            all_response_times.extend([
                result['avg_response_time'],
                result['max_response_time'],
                result['min_response_time']
            ])
        
        global_avg_response_time = sum(all_response_times) / len(all_response_times) if all_response_times else 0
        
        return {
            'summary': {
                'total_logs': total_logs,
                'total_errors': total_errors,
                'error_rate': (total_errors / total_logs * 100) if total_logs > 0 else 0,
                'avg_response_time': global_avg_response_time
            },
            'distributions': {
                'log_levels': dict(global_level_dist),
                'services': dict(global_service_dist),
                'status_codes': dict(global_status_dist)
            },
            'chunks_processed': len(results)
        }
    
    def _print_processing_summary(self, chunks: List[DataChunk]):
        """æ‰“å°å¤„ç†æ‘˜è¦"""
        total_time = self.processing_stats['end_time'] - self.processing_stats['start_time']
        
        successful_chunks = [c for c in chunks if c.processed and not c.error]
        failed_chunks = [c for c in chunks if c.error]
        
        total_processing_time = sum(c.processing_time for c in successful_chunks)
        avg_processing_time = total_processing_time / len(successful_chunks) if successful_chunks else 0
        
        throughput = self.processing_stats['total_records'] / total_time if total_time > 0 else 0
        
        print(f"\nğŸ“Š æ•°æ®å¤„ç†æ‘˜è¦:")
        print(f"  æ€»è®°å½•æ•°: {self.processing_stats['total_records']:,}")
        print(f"  å¤„ç†è®°å½•æ•°: {self.processing_stats['processed_records']:,}")
        print(f"  æˆåŠŸå—æ•°: {len(successful_chunks)}")
        print(f"  å¤±è´¥å—æ•°: {len(failed_chunks)}")
        print(f"  æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"  å¹³å‡å—å¤„ç†æ—¶é—´: {avg_processing_time:.2f}ç§’")
        print(f"  ååé‡: {throughput:.2f} è®°å½•/ç§’")
        
        if self.max_workers > 1:
            theoretical_time = total_processing_time
            parallel_efficiency = (theoretical_time / total_time) / self.max_workers if total_time > 0 else 0
            print(f"  å¹¶è¡Œæ•ˆç‡: {parallel_efficiency:.1%}")
        
        if failed_chunks:
            print(f"\nâŒ å¤±è´¥çš„æ•°æ®å—:")
            for chunk in failed_chunks[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                print(f"    {chunk.chunk_id}: {chunk.error}")


def demo_sales_data_processing():
    """é”€å”®æ•°æ®å¤„ç†æ¼”ç¤º"""
    print("ğŸ›’ é”€å”®æ•°æ®å¤„ç†æ¼”ç¤º")
    print("=" * 60)
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    print("ğŸ“Š ç”Ÿæˆé”€å”®æµ‹è¯•æ•°æ®...")
    sales_data = DataGenerator.generate_sales_data(10000)  # 1ä¸‡æ¡é”€å”®è®°å½•
    
    # åˆ›å»ºæ•°æ®å¤„ç†å™¨
    processor = DataProcessor(max_workers=4, chunk_size=1500)
    
    # å¹¶è¡Œå¤„ç†é”€å”®æ•°æ®
    result = processor.parallel_processing(sales_data, processor.process_sales_analytics)
    
    # æ˜¾ç¤ºåˆ†æç»“æœ
    print(f"\nğŸ“ˆ é”€å”®æ•°æ®åˆ†æç»“æœ:")
    summary = result.get('summary', {})
    print(f"  æ€»è®¢å•æ•°: {summary.get('total_records', 0):,}")
    print(f"  æ€»é”€å”®é¢: Â¥{summary.get('total_amount', 0):,.2f}")
    print(f"  æ€»é”€é‡: {summary.get('total_quantity', 0):,}")
    print(f"  å¹³å‡è®¢å•ä»·å€¼: Â¥{summary.get('avg_order_value', 0):.2f}")
    
    top_performers = result.get('top_performers', {})
    if top_performers:
        top_product = top_performers.get('product', {})
        top_region = top_performers.get('region', {})
        
        print(f"\nğŸ† é”€å”®å† å†›:")
        print(f"  æœ€ä½³äº§å“: {top_product.get('name')} (é”€å”®é¢: Â¥{top_product.get('stats', {}).get('amount', 0):,.2f})")
        print(f"  æœ€ä½³åœ°åŒº: {top_region.get('name')} (é”€å”®é¢: Â¥{top_region.get('stats', {}).get('amount', 0):,.2f})")


def demo_log_data_processing():
    """æ—¥å¿—æ•°æ®å¤„ç†æ¼”ç¤º"""
    print(f"\nğŸ“‹ æ—¥å¿—æ•°æ®å¤„ç†æ¼”ç¤º")
    print("=" * 60)
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    print("ğŸ“Š ç”Ÿæˆæ—¥å¿—æµ‹è¯•æ•°æ®...")
    log_data = DataGenerator.generate_log_data(50000)  # 5ä¸‡æ¡æ—¥å¿—è®°å½•
    
    # åˆ›å»ºæ•°æ®å¤„ç†å™¨
    processor = DataProcessor(max_workers=6, chunk_size=2000)
    
    # å¹¶è¡Œå¤„ç†æ—¥å¿—æ•°æ®
    result = processor.parallel_processing(log_data, processor.process_log_analytics)
    
    # æ˜¾ç¤ºåˆ†æç»“æœ
    print(f"\nğŸ“ˆ æ—¥å¿—åˆ†æç»“æœ:")
    summary = result.get('summary', {})
    print(f"  æ€»æ—¥å¿—æ•°: {summary.get('total_logs', 0):,}")
    print(f"  é”™è¯¯æ—¥å¿—æ•°: {summary.get('total_errors', 0):,}")
    print(f"  é”™è¯¯ç‡: {summary.get('error_rate', 0):.2f}%")
    print(f"  å¹³å‡å“åº”æ—¶é—´: {summary.get('avg_response_time', 0):.2f}ms")
    
    distributions = result.get('distributions', {})
    if distributions:
        print(f"\nğŸ“Š æ—¥å¿—åˆ†å¸ƒ:")
        
        log_levels = distributions.get('log_levels', {})
        print(f"  æ—¥å¿—çº§åˆ«åˆ†å¸ƒ:")
        for level, count in sorted(log_levels.items()):
            percentage = (count / summary.get('total_logs', 1)) * 100
            print(f"    {level}: {count:,} ({percentage:.1f}%)")
        
        services = distributions.get('services', {})
        print(f"  æœåŠ¡åˆ†å¸ƒ:")
        for service, count in sorted(services.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / summary.get('total_logs', 1)) * 100
            print(f"    {service}: {count:,} ({percentage:.1f}%)")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ•°æ®å¤„ç†å™¨å®é™…åº”ç”¨æ¼”ç¤º")
    print("=" * 80)
    
    try:
        demo_sales_data_processing()
        demo_log_data_processing()
        
        print(f"\n{'='*80}")
        print("âœ… æ•°æ®å¤„ç†å™¨æ¼”ç¤ºå®Œæˆ")
        print(f"{'='*80}")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")


if __name__ == "__main__":
    main()