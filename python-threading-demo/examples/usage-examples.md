# Pythonå¤šçº¿ç¨‹æ¼”ç¤ºç³»ç»Ÿä½¿ç”¨ç¤ºä¾‹

æœ¬æ–‡æ¡£æä¾›äº†Pythonå¤šçº¿ç¨‹æ¼”ç¤ºç³»ç»Ÿçš„è¯¦ç»†ä½¿ç”¨ç¤ºä¾‹ã€‚

## ğŸ“‹ ç›®å½•

1. [åŸºç¡€ä½¿ç”¨](#åŸºç¡€ä½¿ç”¨)
2. [é«˜çº§é…ç½®](#é«˜çº§é…ç½®)
3. [å®é™…åº”ç”¨æ¡ˆä¾‹](#å®é™…åº”ç”¨æ¡ˆä¾‹)
4. [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
5. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

## åŸºç¡€ä½¿ç”¨

### å¯åŠ¨æ¼”ç¤ºç³»ç»Ÿ

#### äº¤äº’æ¨¡å¼
```bash
python main.py
```

ç³»ç»Ÿå°†æ˜¾ç¤ºèœå•ï¼Œå¯ä»¥é€‰æ‹©è¿è¡Œç‰¹å®šçš„æ¼”ç¤ºï¼š
```
ğŸ“‹ è¯·é€‰æ‹©è¦è¿è¡Œçš„æ¼”ç¤º:
------------------------------------------------------------
  1. ğŸ§µ åŸºç¡€çº¿ç¨‹æ¼”ç¤º
     å±•ç¤ºPython threadingæ¨¡å—çš„åŸºæœ¬ä½¿ç”¨æ–¹æ³•

  2. âš¡ çº¿ç¨‹æ± æ¼”ç¤º
     å±•ç¤ºconcurrent.futuresæ¨¡å—çš„ThreadPoolExecutorä½¿ç”¨

  3. ğŸ­ ç”Ÿäº§è€…æ¶ˆè´¹è€…æ¼”ç¤º
     å®ç°ç»å…¸çš„ç”Ÿäº§è€…æ¶ˆè´¹è€…æ¨¡å¼ï¼Œå±•ç¤ºçº¿ç¨‹é—´é€šä¿¡

  4. ğŸ”’ çº¿ç¨‹åŒæ­¥æ¼”ç¤º
     å±•ç¤ºå„ç§çº¿ç¨‹åŒæ­¥åŸè¯­çš„ä½¿ç”¨ï¼Œç¡®ä¿çº¿ç¨‹å®‰å…¨

  5. ğŸ“¥ æ–‡ä»¶ä¸‹è½½å™¨
     å¹¶å‘ä¸‹è½½å¤šä¸ªæ–‡ä»¶çš„å®é™…åº”ç”¨åœºæ™¯

  6. ğŸ“Š æ•°æ®å¤„ç†å™¨
     å¤§æ•°æ®é›†å¹¶è¡Œå¤„ç†çš„å®é™…åº”ç”¨åœºæ™¯

  7. ğŸ“‹ æ—¥å¿—åˆ†æå™¨
     æ—¥å¿—æ•°æ®å¹¶è¡Œåˆ†æå¤„ç†

  0. ğŸš€ è¿è¡Œæ‰€æœ‰æ¼”ç¤º
  q. ğŸšª é€€å‡ºç³»ç»Ÿ
------------------------------------------------------------
```

#### å‘½ä»¤è¡Œæ¨¡å¼
```bash
# è¿è¡Œæ‰€æœ‰æ¼”ç¤º
python main.py all

# è¿è¡ŒæŒ‡å®šæ¼”ç¤ºï¼ˆåŸºç¡€çº¿ç¨‹å’Œçº¿ç¨‹æ± ï¼‰
python main.py 1 2

# è¿è¡Œå•ä¸ªæ¼”ç¤º
python main.py 3
```

### è¿è¡Œæµ‹è¯•

```bash
# è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶
python tests/test_suite.py

# è¿è¡Œç‰¹å®šæµ‹è¯•ç±»
python -m unittest tests.test_suite.TestBasicThreadDemo

# è¿è¡Œæµ‹è¯•å¹¶æ˜¾ç¤ºè¯¦ç»†è¾“å‡º
python tests/test_suite.py -v
```

## é«˜çº§é…ç½®

### è‡ªå®šä¹‰é…ç½®æ–‡ä»¶

ç¼–è¾‘ `config/settings.conf` æ¥è°ƒæ•´ç³»ç»Ÿè¡Œä¸ºï¼š

```ini
[system]
# ç³»ç»Ÿé»˜è®¤é…ç½®
default_max_workers = 8      # å¢åŠ é»˜è®¤çº¿ç¨‹æ•°
default_chunk_size = 2000    # å¢åŠ æ•°æ®å—å¤§å°
default_timeout = 60         # å¢åŠ è¶…æ—¶æ—¶é—´

[thread_pool]
# çº¿ç¨‹æ± é…ç½®
max_workers = 8              # æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´
queue_size = 200             # å¢åŠ é˜Ÿåˆ—å¤§å°
timeout_seconds = 60

[file_downloader]
# æ–‡ä»¶ä¸‹è½½å™¨é…ç½®
download_dir = /tmp/downloads # è‡ªå®šä¹‰ä¸‹è½½ç›®å½•
max_workers = 6              # å¢åŠ ä¸‹è½½å¹¶å‘æ•°
chunk_size = 16384           # å¢åŠ ä¸‹è½½å—å¤§å°
retry_attempts = 5           # å¢åŠ é‡è¯•æ¬¡æ•°

[data_processor]
# æ•°æ®å¤„ç†å™¨é…ç½®
max_workers = 8              # å¢åŠ å¤„ç†çº¿ç¨‹æ•°
chunk_size = 5000            # å¢åŠ æ•°æ®å—å¤§å°
enable_monitoring = true     # å¯ç”¨æ€§èƒ½ç›‘æ§
```

### ç¯å¢ƒå˜é‡é…ç½®

```bash
# è®¾ç½®ä¸‹è½½ç›®å½•
export THREADING_DEMO_DOWNLOAD_DIR="/path/to/downloads"

# è®¾ç½®æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
export THREADING_DEMO_MAX_WORKERS=8

# è®¾ç½®è°ƒè¯•æ¨¡å¼
export THREADING_DEMO_DEBUG=true
```

## å®é™…åº”ç”¨æ¡ˆä¾‹

### æ¡ˆä¾‹1ï¼šæ‰¹é‡æ–‡ä»¶ä¸‹è½½

```python
from src.file_downloader import FileDownloader

# åˆ›å»ºä¸‹è½½å™¨
downloader = FileDownloader(download_dir="./downloads", max_workers=4)

# æ·»åŠ ä¸‹è½½ä»»åŠ¡
urls = [
    "https://example.com/file1.zip",
    "https://example.com/file2.pdf",
    "https://example.com/file3.mp4"
]

for i, url in enumerate(urls):
    downloader.add_download(url, f"file_{i+1}")

# æ‰§è¡Œä¸‹è½½
results = downloader.batch_download()

# æ£€æŸ¥ç»“æœ
successful = [r for r in results if r['status'] == 'completed']
print(f"æˆåŠŸä¸‹è½½ {len(successful)} ä¸ªæ–‡ä»¶")
```

### æ¡ˆä¾‹2ï¼šå¤§æ•°æ®é›†å¤„ç†

```python
from src.data_processor import DataProcessor, DataGenerator

# ç”Ÿæˆæµ‹è¯•æ•°æ®
data = DataGenerator.generate_sales_data(100000)  # 10ä¸‡æ¡é”€å”®è®°å½•

# åˆ›å»ºå¤„ç†å™¨
processor = DataProcessor(max_workers=8, chunk_size=5000)

# å¹¶è¡Œå¤„ç†
result = processor.parallel_processing(data, processor.process_sales_analytics)

# æŸ¥çœ‹ç»“æœ
summary = result['summary']
print(f"å¤„ç†äº† {summary['total_records']} æ¡è®°å½•")
print(f"æ€»é”€å”®é¢: Â¥{summary['total_amount']:,.2f}")
```

### æ¡ˆä¾‹3ï¼šè‡ªå®šä¹‰ç”Ÿäº§è€…æ¶ˆè´¹è€…

```python
import queue
import threading
import time

# åˆ›å»ºé˜Ÿåˆ—
task_queue = queue.Queue(maxsize=50)
result_queue = queue.Queue()

def custom_producer(name, count):
    """è‡ªå®šä¹‰ç”Ÿäº§è€…"""
    for i in range(count):
        task = f"{name}_task_{i}"
        task_queue.put(task)
        print(f"ç”Ÿäº§: {task}")
        time.sleep(0.1)

def custom_consumer(name):
    """è‡ªå®šä¹‰æ¶ˆè´¹è€…"""
    while True:
        try:
            task = task_queue.get(timeout=5)
            # æ¨¡æ‹Ÿå¤„ç†
            time.sleep(0.2)
            result = f"processed_{task}"
            result_queue.put(result)
            print(f"[{name}] å¤„ç†: {task} -> {result}")
            task_queue.task_done()
        except queue.Empty:
            break

# åˆ›å»ºå¹¶å¯åŠ¨çº¿ç¨‹
producer_thread = threading.Thread(target=custom_producer, args=("Producer", 20))
consumer_threads = [
    threading.Thread(target=custom_consumer, args=(f"Consumer-{i}",))
    for i in range(3)
]

producer_thread.start()
for t in consumer_threads:
    t.start()

# ç­‰å¾…å®Œæˆ
producer_thread.join()
task_queue.join()

for t in consumer_threads:
    t.join(timeout=1)
```

### æ¡ˆä¾‹4ï¼šçº¿ç¨‹å®‰å…¨çš„è®¡æ•°å™¨

```python
import threading

class ThreadSafeCounter:
    """çº¿ç¨‹å®‰å…¨çš„è®¡æ•°å™¨"""
    
    def __init__(self):
        self._value = 0
        self._lock = threading.Lock()
    
    def increment(self):
        with self._lock:
            self._value += 1
    
    def get_value(self):
        with self._lock:
            return self._value

# ä½¿ç”¨ç¤ºä¾‹
counter = ThreadSafeCounter()

def worker():
    for _ in range(1000):
        counter.increment()

threads = [threading.Thread(target=worker) for _ in range(10)]

for t in threads:
    t.start()

for t in threads:
    t.join()

print(f"æœ€ç»ˆè®¡æ•°: {counter.get_value()}")  # åº”è¯¥æ˜¯10000
```

## æ€§èƒ½ä¼˜åŒ–

### çº¿ç¨‹æ•°é‡ä¼˜åŒ–

```python
import os
import psutil

def get_optimal_thread_count(task_type="io"):
    """è·å–æœ€ä¼˜çº¿ç¨‹æ•°"""
    cpu_count = os.cpu_count()
    
    if task_type == "cpu":
        # CPUå¯†é›†å‹ä»»åŠ¡
        return cpu_count
    elif task_type == "io":
        # IOå¯†é›†å‹ä»»åŠ¡
        return cpu_count * 2
    elif task_type == "mixed":
        # æ··åˆå‹ä»»åŠ¡
        return int(cpu_count * 1.5)
    else:
        return cpu_count

# ä½¿ç”¨ç¤ºä¾‹
optimal_threads = get_optimal_thread_count("io")
print(f"æ¨èçº¿ç¨‹æ•°: {optimal_threads}")
```

### å†…å­˜ä½¿ç”¨ç›‘æ§

```python
import psutil
import threading
import time

def monitor_memory_usage(duration=10):
    """ç›‘æ§å†…å­˜ä½¿ç”¨"""
    process = psutil.Process()
    
    def monitor():
        start_time = time.time()
        while time.time() - start_time < duration:
            memory_info = process.memory_info()
            cpu_percent = process.cpu_percent()
            
            print(f"å†…å­˜ä½¿ç”¨: {memory_info.rss / 1024 / 1024:.1f} MB, "
                  f"CPUä½¿ç”¨: {cpu_percent:.1f}%")
            time.sleep(1)
    
    monitor_thread = threading.Thread(target=monitor)
    monitor_thread.daemon = True
    monitor_thread.start()
    
    return monitor_thread

# å¯åŠ¨ç›‘æ§
monitor_thread = monitor_memory_usage(30)
# ... è¿è¡Œä½ çš„å¤šçº¿ç¨‹ä»£ç  ...
monitor_thread.join()
```

### æ€§èƒ½åŸºå‡†æµ‹è¯•

```python
import time
import concurrent.futures

def benchmark_thread_pool():
    """åŸºå‡†æµ‹è¯•çº¿ç¨‹æ± æ€§èƒ½"""
    
    def cpu_task(n):
        return sum(i * i for i in range(n))
    
    tasks = [5000] * 20
    
    # æµ‹è¯•ä¸åŒçš„çº¿ç¨‹æ•°
    for workers in [1, 2, 4, 8]:
        start_time = time.time()
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
            results = list(executor.map(cpu_task, tasks))
        
        end_time = time.time()
        
        print(f"çº¿ç¨‹æ•° {workers}: {end_time - start_time:.2f}ç§’")

benchmark_thread_pool()
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜è¯Šæ–­

#### é—®é¢˜1ï¼šçº¿ç¨‹æ­»é”
```python
import threading
import time

def detect_deadlock():
    """æ­»é”æ£€æµ‹"""
    lock1 = threading.Lock()
    lock2 = threading.Lock()
    
    def worker1():
        with lock1:
            print("Worker1 è·å¾— lock1")
            time.sleep(0.1)
            print("Worker1 å°è¯•è·å¾— lock2")
            if lock2.acquire(timeout=2):
                try:
                    print("Worker1 è·å¾— lock2")
                finally:
                    lock2.release()
            else:
                print("Worker1 è·å– lock2 è¶…æ—¶ - å¯èƒ½æ­»é”")
    
    def worker2():
        with lock2:
            print("Worker2 è·å¾— lock2")
            time.sleep(0.1)
            print("Worker2 å°è¯•è·å¾— lock1")
            if lock1.acquire(timeout=2):
                try:
                    print("Worker2 è·å¾— lock1")
                finally:
                    lock1.release()
            else:
                print("Worker2 è·å– lock1 è¶…æ—¶ - å¯èƒ½æ­»é”")
    
    t1 = threading.Thread(target=worker1)
    t2 = threading.Thread(target=worker2)
    
    t1.start()
    t2.start()
    
    t1.join()
    t2.join()

detect_deadlock()
```

#### é—®é¢˜2ï¼šå†…å­˜æ³„æ¼æ£€æµ‹
```python
import gc
import threading
import time

def detect_memory_leak():
    """å†…å­˜æ³„æ¼æ£€æµ‹"""
    
    def leaky_function():
        # æ¨¡æ‹Ÿå†…å­˜æ³„æ¼
        data = []
        for i in range(10000):
            data.append(f"data_{i}" * 100)
        # æ•…æ„ä¸æ¸…ç†data
    
    # è¿è¡Œå‰çš„å¯¹è±¡è®¡æ•°
    gc.collect()
    before_count = len(gc.get_objects())
    
    # è¿è¡Œå¯èƒ½æ³„æ¼çš„ä»£ç 
    threads = []
    for i in range(10):
        t = threading.Thread(target=leaky_function)
        threads.append(t)
        t.start()
    
    for t in threads:
        t.join()
    
    # è¿è¡Œåçš„å¯¹è±¡è®¡æ•°
    gc.collect()
    after_count = len(gc.get_objects())
    
    print(f"å¯¹è±¡æ•°é‡å˜åŒ–: {before_count} -> {after_count}")
    print(f"æ–°å¢å¯¹è±¡: {after_count - before_count}")
    
    if after_count - before_count > 1000:
        print("âš ï¸ å¯èƒ½å­˜åœ¨å†…å­˜æ³„æ¼")
    else:
        print("âœ… å†…å­˜ä½¿ç”¨æ­£å¸¸")

detect_memory_leak()
```

#### é—®é¢˜3ï¼šçº¿ç¨‹å¼‚å¸¸å¤„ç†
```python
import threading
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def exception_handler():
    """çº¿ç¨‹å¼‚å¸¸å¤„ç†"""
    
    def risky_function():
        import random
        if random.random() < 0.5:
            raise Exception("éšæœºå¼‚å¸¸")
        return "æˆåŠŸ"
    
    def safe_worker(worker_id):
        try:
            result = risky_function()
            logger.info(f"Worker {worker_id}: {result}")
        except Exception as e:
            logger.error(f"Worker {worker_id} å¼‚å¸¸: {e}")
    
    threads = []
    for i in range(10):
        t = threading.Thread(target=safe_worker, args=(i,))
        threads.append(t)
        t.start()
    
    for t in threads:
        t.join()

exception_handler()
```

### è°ƒè¯•æŠ€å·§

#### çº¿ç¨‹çŠ¶æ€ç›‘æ§
```python
import threading
import time

def monitor_threads():
    """ç›‘æ§çº¿ç¨‹çŠ¶æ€"""
    
    def worker(worker_id, duration):
        print(f"Worker {worker_id} å¼€å§‹")
        time.sleep(duration)
        print(f"Worker {worker_id} å®Œæˆ")
    
    threads = []
    for i in range(5):
        t = threading.Thread(target=worker, args=(i, 2))
        threads.append(t)
        t.start()
    
    # ç›‘æ§çº¿ç¨‹çŠ¶æ€
    while any(t.is_alive() for t in threads):
        active_count = threading.active_count()
        alive_threads = [t.name for t in threads if t.is_alive()]
        print(f"æ´»è·ƒçº¿ç¨‹æ•°: {active_count}, å­˜æ´»çº¿ç¨‹: {alive_threads}")
        time.sleep(0.5)
    
    print("æ‰€æœ‰çº¿ç¨‹å®Œæˆ")

monitor_threads()
```

---

è¿™äº›ç¤ºä¾‹æ¶µç›–äº†ä»åŸºç¡€ä½¿ç”¨åˆ°é«˜çº§é…ç½®çš„å„ç§åœºæ™¯ã€‚æ ¹æ®ä½ çš„å…·ä½“éœ€æ±‚ï¼Œå¯ä»¥é€‰æ‹©ç›¸åº”çš„ç¤ºä¾‹è¿›è¡Œå‚è€ƒå’Œä¿®æ”¹ã€‚