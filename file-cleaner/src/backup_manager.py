#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤‡ä»½æ¨¡å—
æä¾›æ–‡ä»¶å¤‡ä»½å’Œæ¢å¤åŠŸèƒ½ï¼Œæ”¯æŒæ“ä½œå›æ»š
"""

import os
import json
import tarfile
import time
import shutil
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, asdict

from config_manager import get_config
from file_matcher import FileInfo


@dataclass
class BackupMetadata:
    """å¤‡ä»½å…ƒæ•°æ®"""
    backup_id: str                   # å¤‡ä»½ID
    timestamp: float                 # åˆ›å»ºæ—¶é—´æˆ³
    operation_type: str              # æ“ä½œç±»å‹
    original_paths: List[str]        # åŸå§‹æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    file_count: int                  # æ–‡ä»¶æ•°é‡
    total_size: int                  # æ€»æ–‡ä»¶å¤§å°
    backup_path: str                 # å¤‡ä»½æ–‡ä»¶è·¯å¾„
    description: str                 # å¤‡ä»½æè¿°


class BackupManager:
    """å¤‡ä»½ç®¡ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¤‡ä»½ç®¡ç†å™¨"""
        self.config = get_config()
        self.backup_dir = Path(self.config.get('DEFAULT_BACKUP_DIR'))
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        
        # å¤‡ä»½ä¿ç•™å¤©æ•°
        self.max_backup_age = self.config.get_int('MAX_BACKUP_AGE_DAYS', 30)
        
        # å…ƒæ•°æ®æ–‡ä»¶
        self.metadata_file = self.backup_dir / 'backup_index.json'
        
        # åŠ è½½å¤‡ä»½ç´¢å¼•
        self._load_backup_index()
    
    def _load_backup_index(self):
        """åŠ è½½å¤‡ä»½ç´¢å¼•"""
        if self.metadata_file.exists():
            try:
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    self.backup_index = json.load(f)
            except Exception as e:
                print(f"è­¦å‘Š: æ— æ³•åŠ è½½å¤‡ä»½ç´¢å¼•: {e}")
                self.backup_index = {}
        else:
            self.backup_index = {}
    
    def _save_backup_index(self):
        """ä¿å­˜å¤‡ä»½ç´¢å¼•"""
        try:
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.backup_index, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"è­¦å‘Š: æ— æ³•ä¿å­˜å¤‡ä»½ç´¢å¼•: {e}")
    
    def _generate_backup_id(self) -> str:
        """ç”Ÿæˆå¤‡ä»½ID"""
        timestamp = datetime.now()
        return timestamp.strftime("backup_%Y%m%d_%H%M%S")
    
    def create_backup(self, files: List[FileInfo], 
                     description: str = "æ–‡ä»¶æ¸…ç†å¤‡ä»½") -> Optional[str]:
        """
        åˆ›å»ºæ–‡ä»¶å¤‡ä»½
        
        Args:
            files: è¦å¤‡ä»½çš„æ–‡ä»¶åˆ—è¡¨
            description: å¤‡ä»½æè¿°
            
        Returns:
            å¤‡ä»½IDï¼Œå¤±è´¥è¿”å›None
        """
        if not files:
            return None
        
        backup_id = self._generate_backup_id()
        backup_subdir = self.backup_dir / backup_id
        backup_subdir.mkdir(exist_ok=True)
        
        try:
            # åˆ›å»ºå¤‡ä»½å‹ç¼©æ–‡ä»¶
            backup_archive = backup_subdir / "files.tar.gz"
            total_size = 0
            
            with tarfile.open(backup_archive, 'w:gz') as tar:
                for file_info in files:
                    if os.path.exists(file_info.path):
                        # è®¡ç®—ç›¸å¯¹è·¯å¾„ä»¥é¿å…ç»å¯¹è·¯å¾„é—®é¢˜
                        arcname = os.path.relpath(file_info.path, start=os.path.commonpath([f.path for f in files] + [os.getcwd()]))
                        tar.add(file_info.path, arcname=arcname)
                        total_size += file_info.size
            
            # åˆ›å»ºæ–‡ä»¶åˆ—è¡¨
            file_list_path = backup_subdir / "file_list.txt"
            with open(file_list_path, 'w', encoding='utf-8') as f:
                for file_info in files:
                    f.write(f"{file_info.path}\t{file_info.size}\t{file_info.modified_time}\n")
            
            # åˆ›å»ºå…ƒæ•°æ®
            metadata = BackupMetadata(
                backup_id=backup_id,
                timestamp=time.time(),
                operation_type="file_cleanup",
                original_paths=[f.path for f in files],
                file_count=len(files),
                total_size=total_size,
                backup_path=str(backup_archive),
                description=description
            )
            
            # ä¿å­˜å…ƒæ•°æ®åˆ°æ–‡ä»¶
            metadata_path = backup_subdir / "metadata.json"
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(asdict(metadata), f, ensure_ascii=False, indent=2)
            
            # æ›´æ–°å¤‡ä»½ç´¢å¼•
            self.backup_index[backup_id] = asdict(metadata)
            self._save_backup_index()
            
            print(f"âœ… å¤‡ä»½åˆ›å»ºæˆåŠŸ: {backup_id}")
            print(f"   ğŸ“ å¤‡ä»½ä½ç½®: {backup_subdir}")
            print(f"   ğŸ“Š æ–‡ä»¶æ•°é‡: {len(files)}")
            print(f"   ğŸ’¾ æ€»å¤§å°: {self._format_size(total_size)}")
            
            return backup_id
            
        except Exception as e:
            print(f"âŒ å¤‡ä»½åˆ›å»ºå¤±è´¥: {e}")
            # æ¸…ç†å¤±è´¥çš„å¤‡ä»½
            if backup_subdir.exists():
                shutil.rmtree(backup_subdir, ignore_errors=True)
            return None
    
    def list_backups(self) -> List[BackupMetadata]:
        """
        åˆ—å‡ºæ‰€æœ‰å¤‡ä»½
        
        Returns:
            å¤‡ä»½å…ƒæ•°æ®åˆ—è¡¨
        """
        backups = []
        for backup_id, metadata_dict in self.backup_index.items():
            try:
                metadata = BackupMetadata(**metadata_dict)
                backups.append(metadata)
            except Exception as e:
                print(f"è­¦å‘Š: å¤‡ä»½ {backup_id} å…ƒæ•°æ®æŸå: {e}")
        
        # æŒ‰æ—¶é—´æˆ³æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        backups.sort(key=lambda x: x.timestamp, reverse=True)
        return backups
    
    def get_backup_info(self, backup_id: str) -> Optional[BackupMetadata]:
        """
        è·å–å¤‡ä»½ä¿¡æ¯
        
        Args:
            backup_id: å¤‡ä»½ID
            
        Returns:
            å¤‡ä»½å…ƒæ•°æ®ï¼Œä¸å­˜åœ¨è¿”å›None
        """
        if backup_id in self.backup_index:
            try:
                return BackupMetadata(**self.backup_index[backup_id])
            except Exception as e:
                print(f"è­¦å‘Š: å¤‡ä»½ {backup_id} å…ƒæ•°æ®æŸå: {e}")
        return None
    
    def restore_backup(self, backup_id: str, 
                      restore_paths: Optional[List[str]] = None,
                      target_dir: Optional[str] = None) -> bool:
        """
        æ¢å¤å¤‡ä»½
        
        Args:
            backup_id: å¤‡ä»½ID
            restore_paths: è¦æ¢å¤çš„ç‰¹å®šæ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºæ¢å¤å…¨éƒ¨
            target_dir: æ¢å¤ç›®æ ‡ç›®å½•ï¼ŒNoneè¡¨ç¤ºæ¢å¤åˆ°åŸä½ç½®
            
        Returns:
            æ˜¯å¦æˆåŠŸæ¢å¤
        """
        metadata = self.get_backup_info(backup_id)
        if not metadata:
            print(f"âŒ å¤‡ä»½ä¸å­˜åœ¨: {backup_id}")
            return False
        
        backup_archive = Path(metadata.backup_path)
        if not backup_archive.exists():
            print(f"âŒ å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: {backup_archive}")
            return False
        
        try:
            with tarfile.open(backup_archive, 'r:gz') as tar:
                members = tar.getmembers()
                
                # è¿‡æ»¤è¦æ¢å¤çš„æ–‡ä»¶
                if restore_paths:
                    # å°†ç»å¯¹è·¯å¾„è½¬æ¢ä¸ºå‹ç¼©åŒ…å†…çš„ç›¸å¯¹è·¯å¾„
                    restore_names = set()
                    for path in restore_paths:
                        for member in members:
                            if member.name.endswith(os.path.basename(path)):
                                restore_names.add(member.name)
                    
                    members = [m for m in members if m.name in restore_names]
                
                restored_count = 0
                for member in members:
                    try:
                        if target_dir:
                            # æ¢å¤åˆ°æŒ‡å®šç›®å½•
                            extract_path = target_dir
                        else:
                            # æ¢å¤åˆ°åŸä½ç½®ï¼ˆå½“å‰å·¥ä½œç›®å½•ï¼‰
                            extract_path = "."
                        
                        tar.extract(member, path=extract_path)
                        restored_count += 1
                        print(f"  âœ… å·²æ¢å¤: {member.name}")
                        
                    except Exception as e:
                        print(f"  âŒ æ¢å¤å¤±è´¥ {member.name}: {e}")
                
                print(f"ğŸ‰ æ¢å¤å®Œæˆ: {restored_count}/{len(members)} ä¸ªæ–‡ä»¶")
                return restored_count > 0
                
        except Exception as e:
            print(f"âŒ æ¢å¤æ“ä½œå¤±è´¥: {e}")
            return False
    
    def delete_backup(self, backup_id: str) -> bool:
        """
        åˆ é™¤å¤‡ä»½
        
        Args:
            backup_id: å¤‡ä»½ID
            
        Returns:
            æ˜¯å¦æˆåŠŸåˆ é™¤
        """
        if backup_id not in self.backup_index:
            print(f"âŒ å¤‡ä»½ä¸å­˜åœ¨: {backup_id}")
            return False
        
        try:
            backup_subdir = self.backup_dir / backup_id
            if backup_subdir.exists():
                shutil.rmtree(backup_subdir)
            
            # ä»ç´¢å¼•ä¸­ç§»é™¤
            del self.backup_index[backup_id]
            self._save_backup_index()
            
            print(f"âœ… å¤‡ä»½å·²åˆ é™¤: {backup_id}")
            return True
            
        except Exception as e:
            print(f"âŒ åˆ é™¤å¤‡ä»½å¤±è´¥: {e}")
            return False
    
    def cleanup_old_backups(self) -> int:
        """
        æ¸…ç†è¿‡æœŸå¤‡ä»½
        
        Returns:
            æ¸…ç†çš„å¤‡ä»½æ•°é‡
        """
        if self.max_backup_age <= 0:
            return 0
        
        current_time = time.time()
        cutoff_time = current_time - (self.max_backup_age * 24 * 3600)
        
        old_backups = []
        for backup_id, metadata_dict in self.backup_index.items():
            if metadata_dict.get('timestamp', 0) < cutoff_time:
                old_backups.append(backup_id)
        
        cleaned_count = 0
        for backup_id in old_backups:
            if self.delete_backup(backup_id):
                cleaned_count += 1
        
        if cleaned_count > 0:
            print(f"ğŸ§¹ å·²æ¸…ç† {cleaned_count} ä¸ªè¿‡æœŸå¤‡ä»½")
        
        return cleaned_count
    
    def get_backup_statistics(self) -> Dict[str, any]:
        """
        è·å–å¤‡ä»½ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        backups = self.list_backups()
        
        if not backups:
            return {
                "total_backups": 0,
                "total_files": 0,
                "total_size": 0,
                "oldest_backup": None,
                "newest_backup": None,
                "disk_usage": 0
            }
        
        total_files = sum(b.file_count for b in backups)
        total_size = sum(b.total_size for b in backups)
        
        # è®¡ç®—ç£ç›˜ä½¿ç”¨é‡
        disk_usage = 0
        for root, dirs, files in os.walk(self.backup_dir):
            for file in files:
                file_path = os.path.join(root, file)
                try:
                    disk_usage += os.path.getsize(file_path)
                except:
                    pass
        
        return {
            "total_backups": len(backups),
            "total_files": total_files,
            "total_size": total_size,
            "oldest_backup": backups[-1] if backups else None,
            "newest_backup": backups[0] if backups else None,
            "disk_usage": disk_usage
        }
    
    def _format_size(self, size: int) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        if size < 1024:
            return f"{size}B"
        elif size < 1024 * 1024:
            return f"{size/1024:.1f}KB"
        elif size < 1024 * 1024 * 1024:
            return f"{size/(1024*1024):.1f}MB"
        else:
            return f"{size/(1024*1024*1024):.1f}GB"
    
    def _format_time(self, timestamp: float) -> str:
        """æ ¼å¼åŒ–æ—¶é—´"""
        return datetime.fromtimestamp(timestamp).strftime("%Y-%m-%d %H:%M:%S")
    
    def show_backup_list(self):
        """æ˜¾ç¤ºå¤‡ä»½åˆ—è¡¨"""
        backups = self.list_backups()
        
        if not backups:
            print("ğŸ“‹ æ²¡æœ‰æ‰¾åˆ°å¤‡ä»½è®°å½•")
            return
        
        print(f"ğŸ“‹ å¤‡ä»½åˆ—è¡¨ (å…± {len(backups)} ä¸ªå¤‡ä»½):")
        print("=" * 80)
        
        for i, backup in enumerate(backups, 1):
            age_hours = (time.time() - backup.timestamp) / 3600
            
            print(f"{i:2d}. {backup.backup_id}")
            print(f"    ğŸ“… æ—¶é—´: {self._format_time(backup.timestamp)} ({age_hours:.1f}å°æ—¶å‰)")
            print(f"    ğŸ“ æ–‡ä»¶: {backup.file_count} ä¸ª")
            print(f"    ğŸ’¾ å¤§å°: {self._format_size(backup.total_size)}")
            print(f"    ğŸ“ æè¿°: {backup.description}")
            print()
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        stats = self.get_backup_statistics()
        print(f"ğŸ“Š å¤‡ä»½ç»Ÿè®¡:")
        print(f"   æ€»å¤‡ä»½æ•°: {stats['total_backups']}")
        print(f"   æ€»æ–‡ä»¶æ•°: {stats['total_files']}")
        print(f"   ç£ç›˜å ç”¨: {self._format_size(stats['disk_usage'])}")


def create_backup(files: List[FileInfo], description: str = "æ–‡ä»¶æ¸…ç†å¤‡ä»½") -> Optional[str]:
    """
    åˆ›å»ºå¤‡ä»½çš„ä¾¿æ·å‡½æ•°
    
    Args:
        files: è¦å¤‡ä»½çš„æ–‡ä»¶åˆ—è¡¨
        description: å¤‡ä»½æè¿°
        
    Returns:
        å¤‡ä»½IDï¼Œå¤±è´¥è¿”å›None
    """
    manager = BackupManager()
    return manager.create_backup(files, description)


if __name__ == '__main__':
    # æµ‹è¯•å¤‡ä»½åŠŸèƒ½
    manager = BackupManager()
    
    print("=== å¤‡ä»½ç®¡ç†å™¨æµ‹è¯• ===")
    
    # æ˜¾ç¤ºç°æœ‰å¤‡ä»½
    manager.show_backup_list()
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = manager.get_backup_statistics()
    print(f"å¤‡ä»½ç»Ÿè®¡: {stats}")
    
    # æ¸…ç†è¿‡æœŸå¤‡ä»½
    cleaned = manager.cleanup_old_backups()
    if cleaned > 0:
        print(f"æ¸…ç†äº† {cleaned} ä¸ªè¿‡æœŸå¤‡ä»½")
